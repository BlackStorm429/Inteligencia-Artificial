{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\larin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\larin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  class-att  \\\n",
      "0  'BAHIA COCOA REVIEW Showers continued througho...          0   \n",
      "1  'NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESE...          1   \n",
      "2  'ARGENTINE 1986/87 GRAIN/OILSEED REGISTRATIONS...          1   \n",
      "3  'CHAMPION PRODUCTS &lt.CH> APPROVES STOCK SPLI...          0   \n",
      "4  'COMPUTER TERMINAL SYSTEMS &lt.CPML> COMPLETES...          0   \n",
      "\n",
      "                                       ProcessedText  \n",
      "0  bahia cocoa review shower continu throughout w...  \n",
      "1  nation averag price farmer own reserv u agricu...  \n",
      "2  argentin grain oilse registr argentin grain bo...  \n",
      "3  champion product lt ch approv stock split cham...  \n",
      "4  comput termin system lt cpml complet sale comp...  \n",
      "                                                Text  class-att  \\\n",
      "0  'ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN R...          0   \n",
      "1  'CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN ST...          1   \n",
      "2  'JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNW...          0   \n",
      "3  'THAI TRADE DEFICIT WIDENS IN FIRST QUARTER Th...          1   \n",
      "4  'INDONESIA SEES CPO PRICE RISING SHARPLY Indon...          0   \n",
      "\n",
      "                                       ProcessedText  \n",
      "0  asian export fear damag u japan rift mount tra...  \n",
      "1  china daili say vermin eat pct grain stock sur...  \n",
      "2  japan revis long term energi demand downward m...  \n",
      "3  thai trade deficit widen first quarter thailan...  \n",
      "4  indonesia see cpo price rise sharpli indonesia...  \n",
      "Logistic Regression Accuracy: 0.9519867549668874\n",
      "Random Forest Accuracy: 0.9668874172185431\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming NLTK libraries are already installed and imported\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove special characters, numbers, punctuation\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Reconstruct the processed text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Load train and test datasets\n",
    "train_data = pd.read_csv(r'C:\\Users\\larin\\Desktop\\PUC\\IA\\Listas\\Lista 08\\Resolução\\ReutersGrain-train.csv', delimiter=';', encoding='utf-8')\n",
    "test_data = pd.read_csv(r'C:\\Users\\larin\\Desktop\\PUC\\IA\\Listas\\Lista 08\\Resolução\\ReutersGrain-test.csv', delimiter=';', encoding='utf-8')\n",
    "\n",
    "# Apply preprocessing to train and test datasets\n",
    "train_data['ProcessedText'] = train_data['Text'].apply(preprocess)\n",
    "test_data['ProcessedText'] = test_data['Text'].apply(preprocess)\n",
    "\n",
    "# Check the processed data\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Example with max 1000 features\n",
    "\n",
    "# Fit and transform on train set\n",
    "X_train = vectorizer.fit_transform(train_data['ProcessedText']).toarray()\n",
    "y_train = train_data['class-att']\n",
    "\n",
    "# Transform test set (only transform using fitted vectorizer)\n",
    "X_test = vectorizer.transform(test_data['ProcessedText']).toarray()\n",
    "y_test = test_data['class-att']\n",
    "\n",
    "# Initialize models\n",
    "logreg_model = LogisticRegression()\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Train models\n",
    "logreg_model.fit(X_train, y_train)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "logreg_preds = logreg_model.predict(X_test)\n",
    "rf_preds = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_preds)\n",
    "rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "print(f'\\nLogistic Regression Accuracy: {logreg_accuracy}')\n",
    "print(f'\\nRandom Forest Accuracy: {rf_accuracy}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
